{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rH4nP5Kz4VkX"
   },
   "source": [
    "# LPFormer: An Adaptive Graph Transformer for Link Prediction\n",
    "\n",
    "This notebook implements the LPFormer model as described in the paper \"LPFormer: An Adaptive Graph Transformer for Link Prediction\" (Shomer et al., 2024), applied to the Marvel Universe dataset. The implementation includes components described in the paper:\n",
    "\n",
    "1. GCN-based node representation learning\n",
    "2. PPR-based relative positional encodings with order invariance\n",
    "3. GATv2 attention mechanism for adaptive pairwise encoding\n",
    "4. Efficient node selection via PPR thresholding using Andersen's algorithm\n",
    "5. Proper evaluation metrics as specified in the paper\n",
    "6. LP factor analysis for performance evaluation\n",
    "\n",
    "The implementation is optimized for GPU execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Up7Kp08y4VkY"
   },
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRhz_UBt4yOu"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install pyg-lib torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
    "!pip install torch-geometric\n",
    "!pip install numpy pandas scipy matplotlib networkx tqdm scikit-learn\n",
    "!pip install ipywidgets --upgrade\n",
    "!pip show ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0MQzOnZ4VkY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv\n",
    "from torch_geometric.utils import to_undirected, add_self_loops, degree\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.loader import DataLoader\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import norm as sparse_norm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import time\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4-c1l8z4VkZ"
   },
   "source": [
    "## 2. Marvel Dataset Loading and Processing\n",
    "\n",
    "We load and process the Marvel Universe dataset, which consists of connections between heroes and comics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgsof6md4VkZ"
   },
   "outputs": [],
   "source": [
    "print(\"Loading Marvel dataset...\")\n",
    "start_time = time.time()\n",
    "\n",
    "edges_df = pd.read_csv('edges_corr.csv')\n",
    "nodes_df = pd.read_csv('nodes_corr.csv')\n",
    "\n",
    "print(f\"Dataset loaded in {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"Edges shape: {edges_df.shape}\")\n",
    "print(f\"Nodes shape: {nodes_df.shape}\")\n",
    "\n",
    "# Display first few rows of each dataset\n",
    "print(\"\\nEdges preview:\")\n",
    "display(edges_df.head())\n",
    "\n",
    "print(\"\\nNodes preview:\")\n",
    "display(nodes_df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in edges:\", edges_df.isnull().sum().sum())\n",
    "print(\"Missing values in nodes:\", nodes_df.isnull().sum().sum())\n",
    "\n",
    "print(\"\\nUnique heroes:\", edges_df['hero'].nunique())\n",
    "print(\"Unique comics:\", edges_df['comic'].nunique())\n",
    "print(\"Node types:\", nodes_df['type'].unique())\n",
    "print(\"Node type counts:\")\n",
    "display(nodes_df['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy5-Hnwn4VkZ"
   },
   "outputs": [],
   "source": [
    "print(\"Processing Marvel dataset...\")\n",
    "start_time = time.time()\n",
    "\n",
    "node_encoder = LabelEncoder()\n",
    "nodes_df['node_idx'] = node_encoder.fit_transform(nodes_df['node'])\n",
    "\n",
    "node_to_idx = {node: idx for node, idx in zip(nodes_df['node'], nodes_df['node_idx'])}\n",
    "idx_to_node = {idx: node for node, idx in node_to_idx.items()}\n",
    "\n",
    "edges_df['hero_idx'] = edges_df['hero'].map(node_to_idx)\n",
    "edges_df['comic_idx'] = edges_df['comic'].map(node_to_idx)\n",
    "\n",
    "# Check for mapping failures\n",
    "missing_heroes = edges_df[edges_df['hero'].map(lambda x: x not in node_to_idx)]\n",
    "missing_comics = edges_df[edges_df['comic'].map(lambda x: x not in node_to_idx)]\n",
    "\n",
    "if len(missing_heroes) > 0 or len(missing_comics) > 0:\n",
    "    print(f\"Warning: Found {len(missing_heroes)} heroes and {len(missing_comics)} comics missing from nodes_df\")\n",
    "    # Filter out edges with missing nodes\n",
    "    edges_df = edges_df[edges_df['hero'].map(lambda x: x in node_to_idx) &\n",
    "                        edges_df['comic'].map(lambda x: x in node_to_idx)]\n",
    "    \n",
    "    edges_df['hero_idx'] = edges_df['hero'].map(node_to_idx)\n",
    "    edges_df['comic_idx'] = edges_df['comic'].map(node_to_idx)\n",
    "\n",
    "edge_index = torch.tensor([edges_df['hero_idx'].values.astype(np.int64),\n",
    "                          edges_df['comic_idx'].values.astype(np.int64)], dtype=torch.long)\n",
    "\n",
    "# Make the graph undirected for link prediction\n",
    "edge_index = to_undirected(edge_index)\n",
    "\n",
    "nodes_df['type_idx'] = nodes_df['type'].map({'hero': 0, 'comic': 1})\n",
    "node_types = torch.tensor(nodes_df['type_idx'].values, dtype=torch.long)\n",
    "\n",
    "# Create node features\n",
    "# 1. One-hot encoding ensures later no link prediction hero-hero\n",
    "type_features = F.one_hot(node_types, num_classes=2).float()\n",
    "\n",
    "# 2. Add degree features (normalized)\n",
    "row, col = edge_index\n",
    "deg = degree(row, nodes_df.shape[0])\n",
    "deg_normalized = deg / deg.max()\n",
    "deg_features = deg_normalized.unsqueeze(1)\n",
    "\n",
    "node_features = torch.cat([type_features, deg_features], dim=1)\n",
    "\n",
    "data = Data(x=node_features, edge_index=edge_index)\n",
    "data.num_nodes = nodes_df.shape[0]\n",
    "\n",
    "print(f\"Node feature shape: {node_features.shape}\")\n",
    "print(f\"Edge index shape: {edge_index.shape}\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.edge_index.size(1)}\")\n",
    "\n",
    "print(f\"Dataset processing completed in {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_project = time.time()\n",
    "\n",
    "def get_smaller_graph(edge_index, nodes_df, core_ratio=0.1, hops=2, max_ratio=0.3):\n",
    "    \n",
    "    print(f\"\\nStarting graph\")\n",
    "    print(\"(Might take a sec)\")\n",
    "    \n",
    "    node_count = nodes_df.shape[0] \n",
    "    connections_total = edge_index.shape[1] // 2  # Because undirected\n",
    "    target_node_count = int(node_count * max_ratio)\n",
    "    base_nodes = max(100, min(int(node_count * core_ratio), 1000))  # At least 100\n",
    "    \n",
    "    print(f\"\\nOriginal:\")\n",
    "    print(f\"- Nodes: {node_count}\")\n",
    "    print(f\"- Edges: {connections_total}\")\n",
    "    print(f\"- Avg friends: {edge_index.shape[1]/node_count:.1f}\")\n",
    "    print(f\"\\nTrying to get:\")\n",
    "    print(f\"- Start with {base_nodes} base nodes\")\n",
    "    print(f\"- Max {target_node_count} nodes total\")\n",
    "    print(f\"- Looking {hops} steps out\")\n",
    "\n",
    "    neighbors = {} \n",
    "    edge_pairs = set()  # For checking later\n",
    "    \n",
    "    for col in range(edge_index.shape[1]):\n",
    "        node_a = edge_index[0, col].item()\n",
    "        node_b = edge_index[1, col].item()\n",
    "        \n",
    "        if node_a not in neighbors:\n",
    "            neighbors[node_a] = set()\n",
    "        if node_b not in neighbors:\n",
    "            neighbors[node_b] = set()\n",
    "            \n",
    "        neighbors[node_a].add(node_b)\n",
    "        neighbors[node_b].add(node_a)  # Undirected\n",
    "        \n",
    "        # Keep unique pairs\n",
    "        if node_a < node_b:\n",
    "            edge_pairs.add( (node_a, node_b) )\n",
    "        else:\n",
    "            edge_pairs.add( (node_b, node_a) )\n",
    "\n",
    "    node_scores = [ (n, len(neighbors[n])) for n in neighbors ]\n",
    "    node_scores.sort(key=lambda x: -x[1])  # Sort descending\n",
    "\n",
    "    heroes = []\n",
    "    comics = []\n",
    "    for n, score in node_scores:\n",
    "        if n >= len(nodes_df):  # Skip invalid ones\n",
    "            continue\n",
    "        if nodes_df.iloc[n]['type'] == 'hero':\n",
    "            heroes.append( (n, score) )\n",
    "        else:\n",
    "            comics.append( (n, score) )\n",
    "\n",
    "    # Pick starting points - half heroes, half comics\n",
    "    num_hero_starts = min(len(heroes), base_nodes//2)\n",
    "    num_comic_starts = min(len(comics), base_nodes//2)\n",
    "    \n",
    "    start_nodes = [n for n,_ in heroes[:num_hero_starts]]\n",
    "    start_nodes += [n for n,_ in comics[:num_comic_starts]]\n",
    "    current_nodes = set(start_nodes)\n",
    "    all_selected = set(start_nodes)\n",
    "\n",
    "    # Expand in waves\n",
    "    for step in range(hops):\n",
    "        if len(all_selected) >= target_node_count:\n",
    "            print(\"Too big! Stopping early\")\n",
    "            break\n",
    "        \n",
    "        new_nodes = set()\n",
    "        added = 0\n",
    "        \n",
    "        # Look at each node's friends\n",
    "        for node in current_nodes:\n",
    "            if node not in neighbors:  # Just in case\n",
    "                continue\n",
    "                \n",
    "            for friend in neighbors[node]:\n",
    "                if friend not in all_selected:\n",
    "                    if len(all_selected) < target_node_count:\n",
    "                        new_nodes.add(friend)\n",
    "                        all_selected.add(friend)\n",
    "                        added += 1\n",
    "                    else:\n",
    "                        break  # Stop if full\n",
    "                        \n",
    "            if len(all_selected) >= target_node_count:\n",
    "                break  \n",
    "                \n",
    "        current_nodes = new_nodes\n",
    "        \n",
    "        if not new_nodes:\n",
    "            print(\"No more neighbors\")\n",
    "            break\n",
    "\n",
    "    final_nodes = sorted(list(all_selected))\n",
    "    print(f\"\\nDone! Got {len(final_nodes)} nodes\")\n",
    "    print(f\"({len(final_nodes)/node_count:.1%} of original)\")\n",
    "\n",
    "    # Check balance\n",
    "    hero_count = 0\n",
    "    for n in final_nodes:\n",
    "        if n < len(nodes_df) and nodes_df.iloc[n]['type'] == 'hero':\n",
    "            hero_count +=1\n",
    "    print(f\"- Heroes: {hero_count}, Comics: {len(final_nodes)-hero_count}\")\n",
    "\n",
    "    id_map = {old:new for new,old in enumerate(final_nodes)}\n",
    "    \n",
    "    new_edges = []\n",
    "    kept = 0\n",
    "    lost = 0\n",
    "    \n",
    "    for a,b in edge_pairs:\n",
    "        if a in id_map and b in id_map:\n",
    "            new_a = id_map[a]\n",
    "            new_b = id_map[b]\n",
    "            new_edges.append([new_a, new_b])\n",
    "            new_edges.append([new_b, new_a])  # Undirected\n",
    "            kept +=1\n",
    "        else:\n",
    "            lost +=1\n",
    "\n",
    "    if not new_edges:\n",
    "        print(\"Yikes, no edges left!\")\n",
    "        return [], torch.tensor([])\n",
    "    \n",
    "    edge_matrix = torch.tensor(new_edges, dtype=torch.long).t()\n",
    "\n",
    "    if kept > 0:\n",
    "        avg_degree = sum(len(neighbors[n]) for n in final_nodes)/len(final_nodes)\n",
    "        print(f\"\\nNew graph stats:\")\n",
    "        print(f\"- Avg connections: {avg_degree:.1f}\")\n",
    "        print(f\"- Kept {kept/(connections_total):.1%} edges\")\n",
    "\n",
    "    return final_nodes, edge_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Subgraph settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it out with these settings\n",
    "selected_nodes, new_edges = get_smaller_graph(\n",
    "    data.edge_index, nodes_df,\n",
    "    core_ratio=0.1,  \n",
    "    hops=50,          \n",
    "    max_ratio=0.3\n",
    ")\n",
    "\n",
    "if selected_nodes:\n",
    "    remapping = {old:new for new,old in enumerate(selected_nodes)}\n",
    "    \n",
    "    small_features = data.x[selected_nodes]\n",
    "    \n",
    "    compact_data = Data(x=small_features, edge_index=new_edges)\n",
    "    compact_data.num_nodes = len(selected_nodes)\n",
    "    \n",
    "    old_count = len(nodes_df)\n",
    "    new_nodes_df = nodes_df[nodes_df['node_idx'].isin(selected_nodes)].copy()\n",
    "    new_nodes_df['new_id'] = new_nodes_df['node_idx'].map(remapping)\n",
    "    print(f\"Trimmed from {old_count} to {len(new_nodes_df)} nodes\")\n",
    "    \n",
    "    print(f\"\\n\\n ---\")\n",
    "    print(f\"Original: {data.num_nodes} nodes, {data.edge_index.size(1)//2} edges\")\n",
    "    print(f\"New:      {compact_data.num_nodes} nodes, {compact_data.edge_index.size(1)//2} edges\")\n",
    "    print(f\"({compact_data.num_nodes/data.num_nodes:.1%} nodes kept)\")\n",
    "    \n",
    "    data = compact_data\n",
    "    \n",
    "    # Fix the dataframe columns\n",
    "    new_nodes_df['node_idx'] = new_nodes_df['new_id']\n",
    "    new_nodes_df = new_nodes_df.drop('new_id', axis=1)\n",
    "    nodes_df = new_nodes_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nDone in {time.time()-start_time:.2f}s!\")\n",
    "else:\n",
    "    print(\"Failed... using original graph\")\n",
    "\n",
    "node_to_idx = {row['node']:row['node_idx'] for _,row in nodes_df.iterrows()}\n",
    "idx_to_node = {v:k for k,v in node_to_idx.items()}\n",
    "print(f\"Updated mappings for {len(node_to_idx)} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creating train/validation/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Uw_Yx5G4VkZ"
   },
   "outputs": [],
   "source": [
    "print(\"CREATING TRAIN/VALIDATION/TEST SPLITS\")\n",
    "start_time = time.time()\n",
    "\n",
    "transform = RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    is_undirected=True,\n",
    "    add_negative_train_samples=True,\n",
    "    neg_sampling_ratio=1.0\n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "node_type_map = dict(zip(nodes_df['node_idx'], nodes_df['type']))\n",
    "hero_indices = nodes_df[nodes_df['type'] == 'hero']['node_idx'].values\n",
    "comic_indices = nodes_df[nodes_df['type'] == 'comic']['node_idx'].values\n",
    "\n",
    "def create_bipartite_negatives(pos_edges, neg_edges, hero_indices, comic_indices, \n",
    "                               node_type_map, split_name=\"\"):\n",
    "    \n",
    "    print(f\"\\nProcessing {split_name} split:\")\n",
    "    \n",
    "    valid_negatives = []\n",
    "    \n",
    "    for i in range(neg_edges.size(0)):\n",
    "        src, dst = neg_edges[i][0].item(), neg_edges[i][1].item()\n",
    "        \n",
    "        if src in node_type_map and dst in node_type_map:\n",
    "            if node_type_map[src] != node_type_map[dst]:\n",
    "                valid_negatives.append([src, dst])\n",
    "    \n",
    "    filtered_neg_tensor = torch.tensor(valid_negatives, device=neg_edges.device, dtype=neg_edges.dtype) if valid_negatives else torch.zeros((0, 2), device=neg_edges.device, dtype=neg_edges.dtype)\n",
    "    \n",
    "    print(f\"  Filtered negatives: {neg_edges.size(0)} → {len(valid_negatives)} (removed {neg_edges.size(0) - len(valid_negatives)} same-type pairs)\")\n",
    "    \n",
    "    # Calculate how many more negatives we need for 1:1 ratio\n",
    "    n_pos = pos_edges.size(0)\n",
    "    n_neg_current = len(valid_negatives)\n",
    "    n_neg_needed = n_pos - n_neg_current\n",
    "    \n",
    "    if n_neg_needed <= 0:\n",
    "        print(f\"  Already have sufficient negatives ({n_neg_current} neg / {n_pos} pos = {n_neg_current/n_pos:.2f} ratio)\")\n",
    "        return filtered_neg_tensor\n",
    "    \n",
    "    print(f\"  Need {n_neg_needed} more negatives to achieve 1:1 ratio\")\n",
    "    \n",
    "    existing_edges = set()\n",
    "    \n",
    "    for i in range(pos_edges.size(0)):\n",
    "        src, dst = pos_edges[i][0].item(), pos_edges[i][1].item()\n",
    "        existing_edges.add((src, dst))\n",
    "        existing_edges.add((dst, src))  # Undirected\n",
    "    \n",
    "    # Add already selected negative edges\n",
    "    for src, dst in valid_negatives:\n",
    "        existing_edges.add((src, dst))\n",
    "        existing_edges.add((dst, src))\n",
    "    \n",
    "    # Generate new hard negatives\n",
    "    new_negatives = []\n",
    "    attempts = 0\n",
    "    max_attempts = n_neg_needed * 20 \n",
    "    \n",
    "    while len(new_negatives) < n_neg_needed and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        \n",
    "        # Randomly pair a hero with a comic\n",
    "        hero = np.random.choice(hero_indices)\n",
    "        comic = np.random.choice(comic_indices)\n",
    "        \n",
    "        # Check if this edge doesn't exist\n",
    "        if (hero, comic) not in existing_edges:\n",
    "            new_negatives.append([hero, comic])\n",
    "            existing_edges.add((hero, comic))\n",
    "            existing_edges.add((comic, hero))\n",
    "    \n",
    "    print(f\"  Generated {len(new_negatives)} new hard negatives (attempted {attempts} times)\")\n",
    "    \n",
    "    # Combine filtered and new negatives\n",
    "    if new_negatives:\n",
    "        new_neg_tensor = torch.tensor(new_negatives, device=neg_edges.device, dtype=neg_edges.dtype)\n",
    "        if len(valid_negatives) > 0:\n",
    "            combined_negatives = torch.cat([filtered_neg_tensor, new_neg_tensor], dim=0)\n",
    "        else:\n",
    "            combined_negatives = new_neg_tensor\n",
    "    else:\n",
    "        combined_negatives = filtered_neg_tensor\n",
    "    \n",
    "    print(f\"  Final: {n_pos} positives, {combined_negatives.size(0)} negatives (ratio 1:{combined_negatives.size(0)/n_pos:.2f})\")\n",
    "    \n",
    "    return combined_negatives\n",
    "\n",
    "split_edge = {}\n",
    "\n",
    "for split_name, split_data in [('train', train_data), ('valid', val_data), ('test', test_data)]:\n",
    "    # Extract positive and negative edges\n",
    "    pos_mask = split_data.edge_label == 1\n",
    "    neg_mask = split_data.edge_label == 0\n",
    "    \n",
    "    pos_edges = split_data.edge_label_index[:, pos_mask].t()\n",
    "    neg_edges = split_data.edge_label_index[:, neg_mask].t()\n",
    "    \n",
    "    # Process negatives to ensure bipartite structure and 1:1 ratio\n",
    "    balanced_neg_edges = create_bipartite_negatives(\n",
    "        pos_edges, neg_edges, hero_indices, comic_indices, \n",
    "        node_type_map, split_name=split_name\n",
    "    )\n",
    "    \n",
    "    split_edge[split_name] = {\n",
    "        'edge': pos_edges,\n",
    "        'edge_neg': balanced_neg_edges\n",
    "    }\n",
    "\n",
    "print(\"FINAL SPLIT STATISTICS\")\n",
    "\n",
    "total_pos = 0\n",
    "total_neg = 0\n",
    "\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    pos_count = split_edge[split_name]['edge'].size(0)\n",
    "    neg_count = split_edge[split_name]['edge_neg'].size(0)\n",
    "    total_pos += pos_count\n",
    "    total_neg += neg_count\n",
    "    \n",
    "    print(f\"\\n{split_name.upper()} split:\")\n",
    "    print(f\"Positive edges: {pos_count:,}\")\n",
    "    print(f\"Negative edges: {neg_count:,}\")\n",
    "    print(f\"Ratio (pos:neg): 1:{neg_count/pos_count:.2f}\")\n",
    "    \n",
    "    if neg_count > 0:\n",
    "        sample_size = min(5, neg_count)\n",
    "        sample_negs = split_edge[split_name]['edge_neg'][:sample_size]\n",
    "        for i in range(sample_size):\n",
    "            src, dst = sample_negs[i][0].item(), sample_negs[i][1].item()\n",
    "            src_type = node_type_map.get(src, 'unknown')\n",
    "            dst_type = node_type_map.get(dst, 'unknown')\n",
    "\n",
    "print(f\"\\nTotal edges across all splits:\")\n",
    "print(f\"Positive: {total_pos:,}\")\n",
    "print(f\"Negative: {total_neg:,}\")\n",
    "print(f\"Overall ratio: 1:{total_neg/total_pos:.2f}\")\n",
    "\n",
    "print(f\"Data splits created in {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Degree Analyzis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NODE DEGREE ANALYSIS\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def calculate_degrees(edge_index, num_nodes):\n",
    "    if edge_index.is_cuda:\n",
    "        edge_index = edge_index.cpu()\n",
    "    \n",
    "    degrees = torch.zeros(num_nodes, dtype=torch.long)\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        node = edge_index[0, i].item()\n",
    "        degrees[node] += 1\n",
    "    return degrees.numpy()\n",
    "\n",
    "def print_stats(degrees, label):\n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  Count: {len(degrees):,}\")\n",
    "    print(f\"  Mean: {np.mean(degrees):.2f}\")\n",
    "    print(f\"  Median: {np.median(degrees):.2f}\")\n",
    "    print(f\"  Min: {np.min(degrees)}\")\n",
    "    print(f\"  Max: {np.max(degrees)}\")\n",
    "    print(f\"  Std Dev: {np.std(degrees):.2f}\")\n",
    "\n",
    "num_edges = data.edge_index.size(1) // 2\n",
    "print(f\"Graph: {data.num_nodes:,} nodes, {num_edges:,} edges\")\n",
    "\n",
    "degrees = calculate_degrees(data.edge_index, data.num_nodes)\n",
    "\n",
    "hero_indices = nodes_df[nodes_df['type'] == 'hero']['node_idx'].values\n",
    "comic_indices = nodes_df[nodes_df['type'] == 'comic']['node_idx'].values\n",
    "\n",
    "max_idx = len(degrees) - 1\n",
    "hero_indices = hero_indices[hero_indices <= max_idx]\n",
    "comic_indices = comic_indices[comic_indices <= max_idx]\n",
    "\n",
    "hero_degrees = degrees[hero_indices]\n",
    "comic_degrees = degrees[comic_indices]\n",
    "\n",
    "print_stats(degrees, \"OVERALL\")\n",
    "print_stats(hero_degrees, \"HEROES\")\n",
    "print_stats(comic_degrees, \"COMICS\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Node Degree Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "axes[0].hist(degrees, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0].axvline(np.mean(degrees), color='red', linestyle='--', label=f'Mean: {np.mean(degrees):.1f}')\n",
    "axes[0].set_xlabel('Degree')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Overall Degree Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "data_for_box = [hero_degrees, comic_degrees]\n",
    "box_plot = axes[1].boxplot(data_for_box, labels=['Heroes', 'Comics'], patch_artist=True)\n",
    "colors = ['lightblue', 'lightcoral']\n",
    "for patch, color in zip(box_plot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "axes[1].set_ylabel('Degree')\n",
    "axes[1].set_title('Heroes vs Comics')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "top_n = 10\n",
    "top_indices = np.argsort(degrees)[-top_n:][::-1]\n",
    "top_degrees = degrees[top_indices]\n",
    "top_names = [nodes_df.iloc[idx]['node'][:15] + '...' if len(nodes_df.iloc[idx]['node']) > 15 \n",
    "             else nodes_df.iloc[idx]['node'] for idx in top_indices]\n",
    "top_types = [nodes_df.iloc[idx]['type'] for idx in top_indices]\n",
    "\n",
    "colors = ['lightblue' if t == 'hero' else 'lightcoral' for t in top_types]\n",
    "axes[2].barh(range(top_n), top_degrees, color=colors)\n",
    "axes[2].set_yticks(range(top_n))\n",
    "axes[2].set_yticklabels(top_names, fontsize=8)\n",
    "axes[2].set_xlabel('Degree')\n",
    "axes[2].set_title(f'Top {top_n} Highest Degree Nodes')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"KEY INSIGHTS\")\n",
    "\n",
    "density = num_edges / (data.num_nodes * (data.num_nodes - 1) / 2)\n",
    "print(f\"Network Density: {density:.6f} ({density*100:.4f}%)\")\n",
    "\n",
    "hero_mean = np.mean(hero_degrees)\n",
    "comic_mean = np.mean(comic_degrees)\n",
    "print(f\"Average Degree - Heroes: {hero_mean:.2f}, Comics: {comic_mean:.2f}\")\n",
    "\n",
    "if hero_mean > comic_mean * 1.1:\n",
    "    print(\"Heroes tend to be more connected than comics\")\n",
    "elif comic_mean > hero_mean * 1.1:\n",
    "    print(\"Comics tend to be more connected than heroes\")\n",
    "else:\n",
    "    print(\"Heroes and comics have similar connectivity\")\n",
    "\n",
    "hub_threshold = np.percentile(degrees, 95)\n",
    "num_hubs = np.sum(degrees >= hub_threshold)\n",
    "print(f\"Network Hubs (top 5%): {num_hubs} nodes with degree ≥ {hub_threshold:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Subgraph Visualization ( Uncomment the end of the cell in case of use )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_networkx_graph(edge_index, nodes_df):\n",
    "    if edge_index.is_cuda:\n",
    "        edge_index = edge_index.cpu()\n",
    "        \n",
    "    G = nx.Graph()\n",
    "\n",
    "    for idx, row in nodes_df.iterrows():\n",
    "        node_id = row['node_idx']\n",
    "        G.add_node(node_id, \n",
    "                  name=row['node'], \n",
    "                  type=row['type'])\n",
    "    \n",
    "    edge_list = edge_index.t().numpy()\n",
    "    for i in range(0, len(edge_list), 2):  # Skip duplicate edges (undirected)\n",
    "        u, v = edge_list[i]\n",
    "        if not G.has_edge(u, v):\n",
    "            G.add_edge(u, v)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def plot_subgraph(G, nodes_df, layout='spring', figsize=(15, 12)):    \n",
    "    print(f\"Creating visualization with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    fig.suptitle('Subgraph Visualization - Hero-Comic Network', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    if layout == 'spring':\n",
    "        pos = nx.spring_layout(G, k=1, iterations=50, seed=42)\n",
    "    elif layout == 'circular':\n",
    "        pos = nx.circular_layout(G)\n",
    "    elif layout == 'kamada':\n",
    "        pos = nx.kamada_kawai_layout(G)\n",
    "    else:\n",
    "        pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    hero_nodes = [n for n in G.nodes() if G.nodes[n]['type'] == 'hero']\n",
    "    comic_nodes = [n for n in G.nodes() if G.nodes[n]['type'] == 'comic']\n",
    "    \n",
    "    node_colors = []\n",
    "    node_sizes = []\n",
    "    degrees = dict(G.degree())\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        if G.nodes[node]['type'] == 'hero':\n",
    "            node_colors.append('lightblue')\n",
    "        else:\n",
    "            node_colors.append('lightcoral')\n",
    "        \n",
    "        size = max(50, min(500, degrees[node] * 20))\n",
    "        node_sizes.append(size)\n",
    "    \n",
    "    ax1 = axes[0, 0]\n",
    "    nx.draw(G, pos, ax=ax1,\n",
    "            node_color=node_colors,\n",
    "            node_size=node_sizes,\n",
    "            edge_color='gray',\n",
    "            alpha=0.7,\n",
    "            width=0.5)\n",
    "    ax1.set_title(f'Full Subgraph\\n({len(hero_nodes)} heroes, {len(comic_nodes)} comics)')\n",
    "    \n",
    "    legend_elements = [Patch(facecolor='lightblue', label='Heroes'),\n",
    "                      Patch(facecolor='lightcoral', label='Comics')]\n",
    "    ax1.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    ax2 = axes[0, 1]\n",
    "    degree_threshold = np.percentile(list(degrees.values()), 80)  # Top 20%\n",
    "    high_degree_nodes = [n for n, d in degrees.items() if d >= degree_threshold]\n",
    "    G_high = G.subgraph(high_degree_nodes)\n",
    "    \n",
    "    high_pos = {n: pos[n] for n in high_degree_nodes}\n",
    "    high_colors = [node_colors[list(G.nodes()).index(n)] for n in high_degree_nodes]\n",
    "    high_sizes = [node_sizes[list(G.nodes()).index(n)] for n in high_degree_nodes]\n",
    "    \n",
    "    nx.draw(G_high, high_pos, ax=ax2,\n",
    "            node_color=high_colors,\n",
    "            node_size=high_sizes,\n",
    "            edge_color='darkgray',\n",
    "            alpha=0.8,\n",
    "            width=1.0)\n",
    "    ax2.set_title(f'High-Degree Nodes\\n(degree ≥ {degree_threshold:.0f})')\n",
    "    \n",
    "    ax3 = axes[1, 0]\n",
    "    hero_degrees = [degrees[n] for n in hero_nodes]\n",
    "    comic_degrees = [degrees[n] for n in comic_nodes]\n",
    "    \n",
    "    ax3.hist([hero_degrees, comic_degrees], bins=20, alpha=0.7, \n",
    "             label=['Heroes', 'Comics'], color=['lightblue', 'lightcoral'])\n",
    "    ax3.set_xlabel('Degree')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.set_title('Degree Distribution by Type')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax4 = axes[1, 1]\n",
    "    top_n = 15\n",
    "    top_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_node_ids = [n for n, d in top_nodes]\n",
    "    \n",
    "    G_top = G.subgraph(top_node_ids)\n",
    "    top_pos = {n: pos[n] for n in top_node_ids}\n",
    "    top_colors = [node_colors[list(G.nodes()).index(n)] for n in top_node_ids]\n",
    "    \n",
    "    nx.draw(G_top, top_pos, ax=ax4,\n",
    "            node_color=top_colors,\n",
    "            node_size=300,\n",
    "            edge_color='darkgray',\n",
    "            alpha=0.8,\n",
    "            width=1.5)\n",
    "    \n",
    "    labels = {}\n",
    "    for node_id in top_node_ids:\n",
    "        name = G.nodes[node_id]['name']\n",
    "        # Truncate long names\n",
    "        if len(name) > 10:\n",
    "            name = name[:10] + '...'\n",
    "        labels[node_id] = name\n",
    "    \n",
    "    nx.draw_networkx_labels(G_top, top_pos, labels, ax=ax4, font_size=8)\n",
    "    ax4.set_title(f'Top {top_n} Connected Nodes')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return G, pos\n",
    "\"\"\"\n",
    "print(\"Converting to NetworkX format...\")\n",
    "G = create_networkx_graph(data.edge_index, nodes_df)\n",
    "\n",
    "print(f\"\\nSubgraph Statistics:\")\n",
    "print(f\"  Nodes: {G.number_of_nodes():,}\")\n",
    "print(f\"  Edges: {G.number_of_edges():,}\")\n",
    "print(f\"  Average Degree: {2 * G.number_of_edges() / G.number_of_nodes():.2f}\")\n",
    "print(f\"  Density: {nx.density(G):.6f}\")\n",
    "\n",
    "if nx.is_connected(G):\n",
    "    print(f\"  Graph is connected\")\n",
    "    print(f\"  Diameter: {nx.diameter(G)}\")\n",
    "    print(f\"  Average Path Length: {nx.average_shortest_path_length(G):.2f}\")\n",
    "else:\n",
    "    components = list(nx.connected_components(G))\n",
    "    print(f\"  Graph has {len(components)} connected components\")\n",
    "    print(f\"  Largest component: {len(max(components, key=len))} nodes\")\n",
    "\n",
    "# Plot the subgraph\n",
    "print(f\"\\nGenerating visualization...\")\n",
    "G, pos = plot_subgraph(G, nodes_df, layout='spring')\n",
    "\n",
    "# Network analysis\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"NETWORK ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Degree analysis\n",
    "degrees = dict(G.degree())\n",
    "hero_nodes = [n for n in G.nodes() if G.nodes[n]['type'] == 'hero']\n",
    "comic_nodes = [n for n in G.nodes() if G.nodes[n]['type'] == 'comic']\n",
    "\n",
    "hero_degrees = [degrees[n] for n in hero_nodes]\n",
    "comic_degrees = [degrees[n] for n in comic_nodes]\n",
    "\n",
    "print(f\"Heroes: {len(hero_nodes)} nodes, avg degree: {np.mean(hero_degrees):.2f}\")\n",
    "print(f\"Comics: {len(comic_nodes)} nodes, avg degree: {np.mean(comic_degrees):.2f}\")\n",
    "\n",
    "# Top connected nodes\n",
    "print(f\"\\nTop 10 Most Connected Nodes:\")\n",
    "top_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for i, (node_id, degree) in enumerate(top_nodes, 1):\n",
    "    name = G.nodes[node_id]['name']\n",
    "    node_type = G.nodes[node_id]['type']\n",
    "    print(f\"  {i:2d}. {name:25s} (degree: {degree:3d}, type: {node_type})\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbSvAonN4Vka"
   },
   "source": [
    "## 3. PPR Computation using Andersen's Algorithm\n",
    "\n",
    "We implement the efficient Personalized PageRank (PPR) computation using Andersen's algorithm as mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvHOjips4Vka"
   },
   "outputs": [],
   "source": [
    "def compute_ppr_andersen(edge_index, alpha=0.15, eps=1e-5, num_nodes=None):\n",
    "    if num_nodes is None:\n",
    "        num_nodes = edge_index.max().item() + 1\n",
    "\n",
    "    print(f\"Computing PPR matrix for {num_nodes} nodes using Andersen's algorithm...\")\n",
    "    print(f\"This may take a while for large graphs. Please be patient.\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    edge_list = edge_index.t().cpu().numpy()\n",
    "    adj = sp.coo_matrix(\n",
    "        (np.ones(edge_list.shape[0]), (edge_list[:, 0], edge_list[:, 1])),\n",
    "        shape=(num_nodes, num_nodes),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # Make the adjacency matrix symmetric (undirected)\n",
    "    adj = adj + adj.T\n",
    "    adj = adj.tocsr()\n",
    "\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    rowsum[rowsum == 0] = 1.0 \n",
    "    d_inv = np.power(rowsum, -1).flatten()\n",
    "    d_inv[np.isinf(d_inv)] = 0.0\n",
    "    d_mat_inv = sp.diags(d_inv)\n",
    "    norm_adj = d_mat_inv.dot(adj)\n",
    "\n",
    "    ppr_matrix = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
    "\n",
    "    # Progress tracking variables\n",
    "    last_update_time = time.time()\n",
    "    update_interval = 5  # seconds\n",
    "\n",
    "    # Compute PPR for each node using Andersen's algorithm\n",
    "    for i in tqdm(range(num_nodes), desc=\"Computing PPR\"):\n",
    "        # Print progress update every few seconds\n",
    "        current_time = time.time()\n",
    "        if current_time - last_update_time > update_interval:\n",
    "            elapsed = current_time - start_time\n",
    "            progress = (i + 1) / num_nodes\n",
    "            eta = elapsed / progress - elapsed if progress > 0 else 0\n",
    "            #print(f\"Progress: {progress*100:.1f}% ({i+1}/{num_nodes}), Elapsed: {elapsed:.1f}s, ETA: {eta:.1f}s\")\n",
    "            last_update_time = current_time\n",
    "\n",
    "        r = np.zeros(num_nodes)\n",
    "        p = np.zeros(num_nodes)\n",
    "        r[i] = 1.0\n",
    "\n",
    "        while np.max(r) > eps:\n",
    "            j = np.argmax(r)\n",
    "            p[j] += alpha * r[j]\n",
    "\n",
    "            neighbors = norm_adj[j].nonzero()[1]\n",
    "            if len(neighbors) > 0: \n",
    "                for k in neighbors:\n",
    "                    r[k] += (1 - alpha) * r[j] * norm_adj[j, k] / len(neighbors)\n",
    "                    \n",
    "            r[j] = 0\n",
    "\n",
    "        ppr_matrix[i] = p\n",
    "\n",
    "    ppr_tensor = torch.FloatTensor(ppr_matrix)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"PPR matrix computation completed in {total_time:.2f} seconds!\")\n",
    "    return ppr_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sT0w4gAq4Vka"
   },
   "source": [
    "## 4. LPFormer Model Implementation\n",
    "\n",
    "We implement the LPFormer model with all components as described in the paper, including GATv2 attention and order-invariant RPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDtc5aPu4Vka"
   },
   "outputs": [],
   "source": [
    "class PPRThresholding(nn.Module):\n",
    "\n",
    "    # This thresholds should change dynamically but in the paper dont explain how so we will use the init values from the paper\n",
    "    def __init__(self, ppr_matrix, cn_threshold=1e-3, one_hop_threshold=1e-4, multi_hop_threshold=1e-5):\n",
    "        super(PPRThresholding, self).__init__()\n",
    "        self.ppr_matrix = ppr_matrix\n",
    "        self.cn_threshold = cn_threshold\n",
    "        self.one_hop_threshold = one_hop_threshold\n",
    "        self.multi_hop_threshold = multi_hop_threshold\n",
    "        self.last_selection_counts = {}\n",
    "    \n",
    "    def forward(self, src, dst):\n",
    "        src_ppr = self.ppr_matrix[src]\n",
    "        dst_ppr = self.ppr_matrix[dst]\n",
    "        \n",
    "        # Select nodes based on thresholds\n",
    "        ## Common neighbors\n",
    "        cn_mask = (src_ppr > self.cn_threshold) & (dst_ppr > self.cn_threshold)\n",
    "        cn_nodes = torch.nonzero(cn_mask).squeeze(-1)\n",
    "        if cn_nodes.dim() == 0 and cn_nodes.numel() > 0:\n",
    "            cn_nodes = cn_nodes.unsqueeze(0)\n",
    "        \n",
    "        ## One-hop neighbors\n",
    "        one_hop_mask = (src_ppr > self.one_hop_threshold) | (dst_ppr > self.one_hop_threshold)\n",
    "        one_hop_mask = one_hop_mask & ~cn_mask  # Exclude CNs already counted\n",
    "        one_hop_nodes = torch.nonzero(one_hop_mask).squeeze(-1)\n",
    "        if one_hop_nodes.dim() == 0 and one_hop_nodes.numel() > 0:\n",
    "            one_hop_nodes = one_hop_nodes.unsqueeze(0)\n",
    "        \n",
    "        ## Multi-hop neighbors\n",
    "        multi_hop_mask = (src_ppr > self.multi_hop_threshold) | (dst_ppr > self.multi_hop_threshold)\n",
    "        multi_hop_mask = multi_hop_mask & ~cn_mask & ~one_hop_mask  # Exclude already counted nodes\n",
    "        multi_hop_nodes = torch.nonzero(multi_hop_mask).squeeze(-1)\n",
    "        if multi_hop_nodes.dim() == 0 and multi_hop_nodes.numel() > 0:\n",
    "            multi_hop_nodes = multi_hop_nodes.unsqueeze(0)\n",
    "        \n",
    "        self.last_selection_counts = {\n",
    "            'cn': cn_nodes.numel(),\n",
    "            'one_hop': one_hop_nodes.numel(),\n",
    "            'multi_hop': multi_hop_nodes.numel()\n",
    "        }\n",
    "        \n",
    "        selected_nodes = []\n",
    "        if cn_nodes.numel() > 0:\n",
    "            selected_nodes.append(cn_nodes)\n",
    "        if one_hop_nodes.numel() > 0:\n",
    "            selected_nodes.append(one_hop_nodes)\n",
    "        if multi_hop_nodes.numel() > 0:\n",
    "            selected_nodes.append(multi_hop_nodes)\n",
    "        \n",
    "        if selected_nodes:\n",
    "            selected_nodes = torch.cat(selected_nodes)\n",
    "        else:\n",
    "            selected_nodes = torch.tensor([], device=self.ppr_matrix.device, dtype=torch.long)\n",
    "        \n",
    "        if src not in selected_nodes:\n",
    "            selected_nodes = torch.cat([selected_nodes, torch.tensor([src], device=selected_nodes.device)])\n",
    "        if dst not in selected_nodes:\n",
    "            selected_nodes = torch.cat([selected_nodes, torch.tensor([dst], device=selected_nodes.device)])\n",
    "        \n",
    "        return selected_nodes\n",
    "\n",
    "class PPRPositionalEncoding(nn.Module):\n",
    "    def __init__(self, ppr_matrix, hidden_dim):\n",
    "        super(PPRPositionalEncoding, self).__init__()\n",
    "        self.ppr_matrix = ppr_matrix\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.projection = nn.Linear(2, hidden_dim)\n",
    "    \n",
    "    def forward(self, src, dst):\n",
    "        # Get PPR scores between source and destination (bidirectional)\n",
    "        src_to_dst = self.ppr_matrix[src, dst]\n",
    "        dst_to_src = self.ppr_matrix[dst, src]\n",
    "        \n",
    "        # Combine scores in an order-invariant manner\n",
    "        ppr_features = torch.tensor([src_to_dst, dst_to_src], device=self.ppr_matrix.device)\n",
    "        \n",
    "        pos_encoding = self.projection(ppr_features)\n",
    "        \n",
    "        return pos_encoding\n",
    "\n",
    "class GATv2AttentionLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_heads, dropout=0.1):\n",
    "        super(GATv2AttentionLayer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.gat = GATv2Conv(\n",
    "            in_channels=in_dim,\n",
    "            out_channels=out_dim // num_heads,\n",
    "            heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            concat=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        return self.gat(x, edge_index)\n",
    "\n",
    "class LPFormer(nn.Module):\n",
    "    def __init__(self, num_nodes, node_features, train_edge_index, edge_index, hidden_dim=128, num_layers=2, num_heads=4, dropout=0.1, ppr_threshold=1e-3):\n",
    "        super(LPFormer, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.device = node_features.device\n",
    "        self.node_dim = node_features.shape[1]\n",
    "        print(f\"Node feature dimension: {self.node_dim}\")\n",
    "\n",
    "        print(\"Creating GCN layers for node representation...\")\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(self.node_dim, hidden_dim))\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "            print(f\"  Added GCN layer {i+1} with hidden_dim {hidden_dim}\")\n",
    "\n",
    "        print(\"Computing PPR matrix using Andersen's algorithm...\")\n",
    "        ppr_tensor = compute_ppr_andersen(\n",
    "            train_data.edge_index,\n",
    "            alpha=0.15,\n",
    "            eps=1e-5,\n",
    "            num_nodes=num_nodes\n",
    "        )\n",
    "\n",
    "        ppr_tensor = ppr_tensor.to(self.device)\n",
    "        self.register_buffer('ppr_matrix', ppr_tensor)\n",
    "        print(f\"PPR matrix shape: {ppr_tensor.shape}, device: {ppr_tensor.device}\")\n",
    "\n",
    "        print(\"Creating adjacency matrix...\")\n",
    "        edge_list = train_data.edge_index.t().cpu().numpy()\n",
    "        adj = sp.coo_matrix(\n",
    "            (np.ones(edge_list.shape[0]), (edge_list[:, 0], edge_list[:, 1])),\n",
    "            shape=(num_nodes, num_nodes),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        adj_tensor = torch.FloatTensor(adj.todense()).to(self.device)\n",
    "        self.register_buffer('adj_matrix', adj_tensor)\n",
    "        print(f\"Adjacency matrix shape: {adj_tensor.shape}, device: {adj_tensor.device}\")\n",
    "\n",
    "        print(\"Creating PPR thresholding module...\")\n",
    "        cn_threshold = ppr_threshold\n",
    "        one_hop_threshold = ppr_threshold / 10\n",
    "        multi_hop_threshold = ppr_threshold / 100\n",
    "        self.ppr_threshold = PPRThresholding(\n",
    "            self.ppr_matrix,\n",
    "            cn_threshold=cn_threshold,\n",
    "            one_hop_threshold=one_hop_threshold,\n",
    "            multi_hop_threshold=multi_hop_threshold\n",
    "        )\n",
    "\n",
    "        print(\"Creating PPR positional encoding module with order invariance...\")\n",
    "        self.ppr_pos_encoding = PPRPositionalEncoding(self.ppr_matrix, hidden_dim)\n",
    "\n",
    "        print(\"Creating GATv2 attention layers...\")\n",
    "        self.attention_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.attention_layers.append(GATv2AttentionLayer(hidden_dim, hidden_dim, num_heads, dropout))\n",
    "            print(f\"  Added GATv2 attention layer {i+1}\")\n",
    "\n",
    "        print(\"Creating final prediction layer...\")\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + 3, hidden_dim),  # node product + pairwise + 3 counts\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        print(\"LPFormer model initialized successfully!\")\n",
    "        print(\"---\\n\")\n",
    "\n",
    "    def forward(self, node_features, edge_index, target_links):\n",
    "        node_features = node_features.to(self.device)\n",
    "        edge_index = edge_index.to(self.device)\n",
    "        target_links = target_links.to(self.device)\n",
    "\n",
    "        total_cn_count = 0\n",
    "        total_one_hop_count = 0\n",
    "        total_multi_hop_count = 0\n",
    "        total_nodes = 0\n",
    "        \n",
    "        x = node_features\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.convs) - 1: \n",
    "                x = F.relu(x)\n",
    "    \n",
    "        predictions = []\n",
    "    \n",
    "        #target_links_iter = tqdm(target_links, desc=\"Processing links\", disable=len(target_links) < 100)\n",
    "        target_links_iter = tqdm(target_links, desc=\"Processing links\", disable=True)\n",
    "    \n",
    "        for link in target_links_iter:\n",
    "            src, dst = link\n",
    "    \n",
    "            selected_nodes = self.ppr_threshold(src, dst)\n",
    "            if hasattr(self.ppr_threshold, 'last_selection_counts'):\n",
    "                total_cn_count += self.ppr_threshold.last_selection_counts.get('cn', 0)\n",
    "                total_one_hop_count += self.ppr_threshold.last_selection_counts.get('one_hop', 0)\n",
    "                total_multi_hop_count += self.ppr_threshold.last_selection_counts.get('multi_hop', 0)\n",
    "                total_nodes += len(selected_nodes)\n",
    "           \n",
    "            # First, get neighbors of both source and destination\n",
    "            src_neighbors = set(self.adj_matrix[src].nonzero(as_tuple=False).flatten().cpu().numpy())\n",
    "            dst_neighbors = set(self.adj_matrix[dst].nonzero(as_tuple=False).flatten().cpu().numpy())\n",
    "            \n",
    "            # Categorize each selected node\n",
    "            cn_count = 0\n",
    "            one_hop_count = 0\n",
    "            multi_hop_count = 0\n",
    "            \n",
    "            for node in selected_nodes.cpu().numpy():\n",
    "                # Skip source and destination nodes\n",
    "                if node == src.item() or node == dst.item():\n",
    "                    continue\n",
    "                    \n",
    "                if node in src_neighbors and node in dst_neighbors:\n",
    "                    cn_count += 1\n",
    "                elif node in src_neighbors or node in dst_neighbors:\n",
    "                    one_hop_count += 1\n",
    "                else:\n",
    "                    multi_hop_count += 1\n",
    "            \n",
    "            if link is target_links[0] or link is target_links[min(10, len(target_links)-1)] or link is target_links[min(100, len(target_links)-1)]:\n",
    "                print(f\"\\nNode selection for link {src.item()}-{dst.item()}:\")\n",
    "                print(f\"  Selected {len(selected_nodes)} nodes: {cn_count} CNs, {one_hop_count} 1-hops, {multi_hop_count} multi-hops\")\n",
    "                print(f\"  Ratio: {cn_count/(cn_count+one_hop_count+multi_hop_count):.2%} CNs, {one_hop_count/(cn_count+one_hop_count+multi_hop_count):.2%} 1-hops, {multi_hop_count/(cn_count+one_hop_count+multi_hop_count):.2%} multi-hops\")\n",
    "            \n",
    "            subgraph_x = x[selected_nodes]\n",
    "    \n",
    "            n = len(selected_nodes)\n",
    "            rows, cols = [], []\n",
    "            for a in range(n):\n",
    "                for b in range(n):\n",
    "                    if a != b:  # Exclude self-loops\n",
    "                        rows.append(a)\n",
    "                        cols.append(b)\n",
    "            subgraph_edge_index = torch.tensor([rows, cols], dtype=torch.long, device=self.device)\n",
    "    \n",
    "            for attn_layer in self.attention_layers:\n",
    "                subgraph_x = attn_layer(subgraph_x, subgraph_edge_index)\n",
    "                subgraph_x = F.relu(subgraph_x)\n",
    "                subgraph_x = F.dropout(subgraph_x, p=self.dropout, training=self.training)\n",
    "    \n",
    "            src_idx = (selected_nodes == src).nonzero().item()\n",
    "            dst_idx = (selected_nodes == dst).nonzero().item()\n",
    "    \n",
    "            src_repr = subgraph_x[src_idx]\n",
    "            dst_repr = subgraph_x[dst_idx]\n",
    "    \n",
    "            # Compute PPR-based positional encoding\n",
    "            pos_encoding = self.ppr_pos_encoding(src, dst)\n",
    "    \n",
    "            # Compute LP factors\n",
    "            ## Common neighbors count\n",
    "            def get_common_neighbors(adj_matrix, src, dst):\n",
    "                src_row = src.item()\n",
    "                dst_row = dst.item()\n",
    "                src_neighbors = adj_matrix[src_row].nonzero(as_tuple=False).flatten()\n",
    "                dst_neighbors = adj_matrix[dst_row].nonzero(as_tuple=False).flatten()\n",
    "                \n",
    "                # Handle empty neighbor cases\n",
    "                if src_neighbors.shape[0] == 0 or dst_neighbors.shape[0] == 0:\n",
    "                    return torch.tensor(0, device=adj_matrix.device).float()\n",
    "                \n",
    "                # Convert to sets for intersection\n",
    "                src_set = set(src_neighbors.cpu().numpy())\n",
    "                dst_set = set(dst_neighbors.cpu().numpy())\n",
    "                common_count = len(src_set.intersection(dst_set))\n",
    "                \n",
    "                return torch.tensor(common_count, device=adj_matrix.device).float()\n",
    "            \n",
    "            # Calculate common neighbors\n",
    "            common_neighbors = get_common_neighbors(self.adj_matrix, src, dst)\n",
    "            common_neighbors = common_neighbors / (self.num_nodes ** 0.5)  # Normalize\n",
    "    \n",
    "            ## PPR score\n",
    "            src_ppr = self.ppr_matrix[src]\n",
    "            dst_ppr = self.ppr_matrix[dst]\n",
    "            ppr_sim = F.cosine_similarity(src_ppr.unsqueeze(0), dst_ppr.unsqueeze(0)).item()\n",
    "            ppr_score = torch.tensor(ppr_sim, device=self.device)\n",
    "            \n",
    "            ## Feature similarity\n",
    "            feat_sim = F.cosine_similarity(node_features[src].unsqueeze(0), node_features[dst].unsqueeze(0)).item()\n",
    "            feat_sim = torch.tensor(feat_sim, device=self.device)\n",
    "    \n",
    "            # Combine node representations and LP factors\n",
    "            combined_repr = torch.cat([\n",
    "                src_repr * dst_repr,\n",
    "                pos_encoding,\n",
    "                common_neighbors.unsqueeze(0),\n",
    "                ppr_score.unsqueeze(0),\n",
    "                feat_sim.unsqueeze(0)\n",
    "            ])\n",
    "    \n",
    "            pred = self.predictor(combined_repr)\n",
    "            predictions.append(pred)\n",
    "\n",
    "        return torch.cat(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ow-Yx5G4Vkb"
   },
   "source": [
    "## 5. Evaluation Metrics\n",
    "\n",
    "We implement the evaluation metrics used in the paper, including Mean Reciprocal Rank (MRR), AUC, and Average Precision (AP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Uw_Yx5G4Vkb"
   },
   "outputs": [],
   "source": [
    "class MRREvaluator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def eval(self, input_dict):\n",
    "        y_pred_pos = input_dict['y_pred_pos']\n",
    "        y_pred_neg = input_dict['y_pred_neg']\n",
    "\n",
    "        # Compute MRR\n",
    "        mrr_list = []\n",
    "        \n",
    "        for i, pos_score in enumerate(y_pred_pos):\n",
    "            if isinstance(y_pred_neg, list):\n",
    "                if i < len(y_pred_neg):\n",
    "                    neg_scores = y_pred_neg[i]\n",
    "                else:\n",
    "                    print(f\"Warning: Not enough negative scores for positive example {i}\")\n",
    "                    continue\n",
    "            else:\n",
    "                # Assume y_pred_neg is a tensor with all negatives\n",
    "                batch_size = y_pred_neg.shape[0] // y_pred_pos.shape[0]\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, y_pred_neg.shape[0])\n",
    "                neg_scores = y_pred_neg[start_idx:end_idx]\n",
    "\n",
    "            all_scores = torch.cat([pos_score.view(1), neg_scores])\n",
    "            \n",
    "            sorted_indices = torch.argsort(all_scores, descending=True)\n",
    "            \n",
    "            rank = (sorted_indices == 0).nonzero().item() + 1\n",
    "            \n",
    "            # Compute reciprocal rank\n",
    "            mrr_list.append(1.0 / rank)\n",
    "        \n",
    "        return torch.tensor(mrr_list).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ow-Yx5G4Vkc"
   },
   "source": [
    "## 6. Training, Evaluation and Test Functions\n",
    "\n",
    "We implement the training and evaluation functions for the LPFormer model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Uw_Yx5G4Vkc"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, data, split_edge, batch_size=1024):\n",
    "    model.train()\n",
    "    device = model.device\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_edge = split_edge['train']['edge'].to(device)\n",
    "    train_edge_neg = split_edge['train']['edge_neg'].to(device)\n",
    "    \n",
    "    train_edge_all = torch.cat([train_edge, train_edge_neg], dim=0)\n",
    "    train_label_all = torch.cat([torch.ones(train_edge.size(0)), torch.zeros(train_edge_neg.size(0))], dim=0).to(device)\n",
    "    \n",
    "    perm = torch.randperm(train_edge_all.size(0))\n",
    "    train_edge_all = train_edge_all[perm]\n",
    "    train_label_all = train_label_all[perm]\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = (train_edge_all.size(0) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"Training batches\"):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, train_edge_all.size(0))\n",
    "        batch_edge = train_edge_all[start_idx:end_idx]\n",
    "        batch_label = train_label_all[start_idx:end_idx]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(data.x, train_data.edge_index, batch_edge)\n",
    "        loss = F.binary_cross_entropy(pred, batch_label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * batch_edge.size(0)\n",
    "    \n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    return total_loss / train_edge_all.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ygKJYDl4Vkc"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, split_edge, evaluator, batch_size=1024):\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"Evaluating on validation set...\")\n",
    "    pos_valid_edge = split_edge['valid']['edge'].to(device)\n",
    "    neg_valid_edge = split_edge['valid']['edge_neg'].to(device)\n",
    "    \n",
    "    pos_valid_preds = []\n",
    "    for perm in DataLoader(range(pos_valid_edge.size(0)), batch_size):\n",
    "        edge = pos_valid_edge[perm]\n",
    "        pos_valid_preds.append(model(data.x, data.edge_index, edge).cpu())\n",
    "    pos_valid_pred = torch.cat(pos_valid_preds, dim=0)\n",
    "    \n",
    "    neg_valid_preds = []\n",
    "    for perm in DataLoader(range(neg_valid_edge.size(0)), batch_size):\n",
    "        edge = neg_valid_edge[perm]\n",
    "        neg_valid_preds.append(model(data.x, data.edge_index, edge).cpu())\n",
    "    neg_valid_pred = torch.cat(neg_valid_preds, dim=0)\n",
    "    \n",
    "    print(\"Evaluating on test set...\")\n",
    "    pos_test_edge = split_edge['test']['edge'].to(device)\n",
    "    neg_test_edge = split_edge['test']['edge_neg'].to(device)\n",
    "    \n",
    "    pos_test_preds = []\n",
    "    for perm in DataLoader(range(pos_test_edge.size(0)), batch_size):\n",
    "        edge = pos_test_edge[perm]\n",
    "        pos_test_preds.append(model(data.x, data.edge_index, edge).cpu())\n",
    "    pos_test_pred = torch.cat(pos_test_preds, dim=0)\n",
    "    \n",
    "    neg_test_preds = []\n",
    "    for perm in DataLoader(range(neg_test_edge.size(0)), batch_size):\n",
    "        edge = neg_test_edge[perm]\n",
    "        neg_test_preds.append(model(data.x, data.edge_index, edge).cpu())\n",
    "    neg_test_pred = torch.cat(neg_test_preds, dim=0)\n",
    "    \n",
    "    print(\"Computing evaluation metrics...\")\n",
    "    results = {}\n",
    "    \n",
    "    # Prepare data for MRR evaluation\n",
    "    valid_mrr_data = {\n",
    "        'y_pred_pos': pos_valid_pred,\n",
    "        'y_pred_neg': []\n",
    "    }\n",
    "    \n",
    "    neg_per_pos = neg_valid_edge.size(0) // pos_valid_edge.size(0)\n",
    "    for i in range(pos_valid_edge.size(0)):\n",
    "        start_idx = i * neg_per_pos\n",
    "        end_idx = start_idx + neg_per_pos\n",
    "        # Handle the case where division isn't perfect\n",
    "        if i == pos_valid_edge.size(0) - 1:\n",
    "            end_idx = neg_valid_edge.size(0)\n",
    "        valid_mrr_data['y_pred_neg'].append(neg_valid_pred[start_idx:end_idx])\n",
    "    \n",
    "    test_mrr_data = {\n",
    "        'y_pred_pos': pos_test_pred,\n",
    "        'y_pred_neg': []\n",
    "    }\n",
    "    \n",
    "    neg_per_pos = neg_test_edge.size(0) // pos_test_edge.size(0)\n",
    "    for i in range(pos_test_edge.size(0)):\n",
    "        start_idx = i * neg_per_pos\n",
    "        end_idx = start_idx + neg_per_pos\n",
    "        # Handle the case where division isn't perfect\n",
    "        if i == pos_test_edge.size(0) - 1:\n",
    "            end_idx = neg_test_edge.size(0)\n",
    "        test_mrr_data['y_pred_neg'].append(neg_test_pred[start_idx:end_idx])\n",
    "    \n",
    "    # Compute MRR\n",
    "    valid_mrr = evaluator.eval(valid_mrr_data)\n",
    "    test_mrr = evaluator.eval(test_mrr_data)\n",
    "    \n",
    "    # Compute AUC and AP\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    \n",
    "    valid_labels = torch.cat([torch.ones(pos_valid_pred.size(0)), torch.zeros(neg_valid_pred.size(0))]).numpy()\n",
    "    valid_preds = torch.cat([pos_valid_pred, neg_valid_pred]).numpy()\n",
    "    valid_auc = roc_auc_score(valid_labels, valid_preds)\n",
    "    valid_ap = average_precision_score(valid_labels, valid_preds)\n",
    "    \n",
    "    test_labels = torch.cat([torch.ones(pos_test_pred.size(0)), torch.zeros(neg_test_pred.size(0))]).numpy()\n",
    "    test_preds = torch.cat([pos_test_pred, neg_test_pred]).numpy()\n",
    "    test_auc = roc_auc_score(test_labels, test_preds)\n",
    "    test_ap = average_precision_score(test_labels, test_preds)\n",
    "    \n",
    "    results['valid'] = valid_mrr\n",
    "    results['test'] = test_mrr\n",
    "    results['valid_auc'] = valid_auc\n",
    "    results['test_auc'] = test_auc\n",
    "    results['valid_ap'] = valid_ap\n",
    "    results['test_ap'] = test_ap\n",
    "    \n",
    "    evaluation_time = time.time() - start_time\n",
    "    print(f\"Evaluation completed in {evaluation_time:.2f} seconds\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BrIv15AbGn3"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def analyze_lp_factors(model, data, split_edge, percentile=90):\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    \n",
    "    print(\"Analyzing LP factors...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pos_test_edge = split_edge['test']['edge'].to(device)\n",
    "    \n",
    "    def get_neighbors(node_idx):\n",
    "        neighbors = model.adj_matrix[node_idx].nonzero(as_tuple=False).flatten()\n",
    "        return set(neighbors.cpu().numpy()) if neighbors.numel() > 0 else set()\n",
    "    \n",
    "    # Compute LP factors for each edge\n",
    "    # Common neighbors\n",
    "    print(\"Computing common neighbor scores...\")\n",
    "    cn_scores = []\n",
    "    for edge in tqdm(pos_test_edge, desc=\"Computing CN scores\"):\n",
    "        a, b = edge[0].item(), edge[1].item()\n",
    "        a_neighbors = get_neighbors(a)\n",
    "        b_neighbors = get_neighbors(b)\n",
    "        cn_score = len(a_neighbors & b_neighbors)\n",
    "        cn_scores.append(cn_score)\n",
    "    cn_scores = torch.tensor(cn_scores, device=device).float()  # Convert to float\n",
    "    \n",
    "    # PPR\n",
    "    print(\"Computing PPR scores...\")\n",
    "    ppr_scores = []\n",
    "    for edge in tqdm(pos_test_edge, desc=\"Computing PPR scores\"):\n",
    "        a, b = edge[0].item(), edge[1].item()\n",
    "        \n",
    "        # Use cosine similarity of PPR vectors for a more robust score\n",
    "        src_ppr = model.ppr_matrix[a]\n",
    "        dst_ppr = model.ppr_matrix[b]\n",
    "        ppr_sim = F.cosine_similarity(src_ppr.unsqueeze(0), dst_ppr.unsqueeze(0)).item()\n",
    "        ppr_scores.append(ppr_sim)\n",
    "    ppr_scores = torch.tensor(ppr_scores, device=device).float()  # Convert to float\n",
    "    \n",
    "    # Cosine similarity\n",
    "    print(\"Computing feature similarity scores...\")\n",
    "    feat_scores = []\n",
    "    for edge in tqdm(pos_test_edge, desc=\"Computing feature similarity scores\"):\n",
    "        a, b = edge[0].item(), edge[1].item()\n",
    "        feat_a = data.x[a]\n",
    "        feat_b = data.x[b]\n",
    "        feat_sim = F.cosine_similarity(feat_a.unsqueeze(0), feat_b.unsqueeze(0)).item()\n",
    "        feat_scores.append(feat_sim)\n",
    "    feat_scores = torch.tensor(feat_scores, device=device).float()  # Convert to float\n",
    "\n",
    "    # Values from the paper\n",
    "    cn_threshold = torch.quantile(cn_scores, 0.75)  # 75th percentile for CN\n",
    "    ppr_threshold = torch.quantile(ppr_scores, 0.85)  # 85th percentile for PPR\n",
    "    feat_threshold = torch.quantile(feat_scores, 0.75)  # 75th percentile for features\n",
    "    \n",
    "    print(f\"Adjusted percentile thresholds:\")\n",
    "    print(f\"  CN (75th): {cn_threshold:.4f}\")\n",
    "    print(f\"  PPR (85th): {ppr_threshold:.4f}\")\n",
    "    print(f\"  Feature (75th): {feat_threshold:.4f}\")\n",
    "    \n",
    "    print(\"\\nScore distributions:\")\n",
    "    def print_histogram(scores, name, bins=5):\n",
    "        import numpy as np\n",
    "        counts, bin_edges = np.histogram(scores.cpu().numpy(), bins=bins)\n",
    "        max_count = max(counts)\n",
    "        bar_length = 30 \n",
    "        \n",
    "        print(f\"\\n{name} distribution:\")\n",
    "        for i in range(len(counts)):\n",
    "            bar = \"#\" * int(counts[i] / max_count * bar_length)\n",
    "            print(f\"  [{bin_edges[i]:.4f}, {bin_edges[i+1]:.4f}): {counts[i]:5d} {bar}\")\n",
    "    \n",
    "    print_histogram(cn_scores, \"Common Neighbors\")\n",
    "    print_histogram(ppr_scores, \"PPR\")\n",
    "    print_histogram(feat_scores, \"Feature Similarity\")\n",
    "    \n",
    "    print(\"\\nCategorizing edges by dominant factor...\")\n",
    "    local_edges = []\n",
    "    global_edges = []\n",
    "    feature_edges = []\n",
    "    \n",
    "    for i, edge in enumerate(pos_test_edge):\n",
    "        # Calculate relative strength of each factor compared to its threshold\n",
    "        epsilon = 1e-6\n",
    "        rel_local = cn_scores[i] / (cn_threshold + epsilon) if cn_threshold > 0 else 0\n",
    "        rel_global = ppr_scores[i] / (ppr_threshold + epsilon)\n",
    "        rel_feature = feat_scores[i] / (feat_threshold + epsilon)\n",
    "        \n",
    "        # Find dominant factor\n",
    "        rel_scores = [rel_local, rel_global, rel_feature]\n",
    "        max_rel = max(rel_scores)\n",
    "        dominant_idx = rel_scores.index(max_rel)\n",
    "        \n",
    "        # Only categorize if the dominant factor exceeds its threshold\n",
    "        if max_rel >= 1.0:\n",
    "            if dominant_idx == 0:\n",
    "                local_edges.append(i)\n",
    "            elif dominant_idx == 1:\n",
    "                global_edges.append(i)\n",
    "            elif dominant_idx == 2:\n",
    "                feature_edges.append(i)\n",
    "    \n",
    "    local_edges = torch.tensor(local_edges, device=device)\n",
    "    global_edges = torch.tensor(global_edges, device=device)\n",
    "    feature_edges = torch.tensor(feature_edges, device=device)\n",
    "    \n",
    "    print(f\"Edges categorized by dominant factor:\")\n",
    "    print(f\"  Local: {len(local_edges)}\")\n",
    "    print(f\"  Global: {len(global_edges)}\")\n",
    "    print(f\"  Feature: {len(feature_edges)}\")\n",
    "    print(f\"  Total categorized: {len(local_edges) + len(global_edges) + len(feature_edges)}\")\n",
    "    print(f\"  Total test edges: {len(pos_test_edge)}\")\n",
    "    \n",
    "    print(\"Evaluating model performance by factor type...\")\n",
    "    results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if len(local_edges) > 0:\n",
    "            print(\"Evaluating on local factor edges...\")\n",
    "            local_pred = model(data.x, data.edge_index, pos_test_edge[local_edges])\n",
    "            results['local'] = local_pred.mean().item()\n",
    "        else:\n",
    "            results['local'] = float('nan')\n",
    "        \n",
    "        if len(global_edges) > 0:\n",
    "            print(\"Evaluating on global factor edges...\")\n",
    "            global_pred = model(data.x, data.edge_index, pos_test_edge[global_edges])\n",
    "            results['global'] = global_pred.mean().item()\n",
    "        else:\n",
    "            results['global'] = float('nan')\n",
    "        \n",
    "        if len(feature_edges) > 0:\n",
    "            print(\"Evaluating on feature factor edges...\")\n",
    "            feature_pred = model(data.x, data.edge_index, pos_test_edge[feature_edges])\n",
    "            results['feature'] = feature_pred.mean().item()\n",
    "        else:\n",
    "            results['feature'] = float('nan')\n",
    "    \n",
    "    analysis_time = time.time() - start_time\n",
    "    print(f\"LP factor analysis completed in {analysis_time:.2f} seconds\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORvxQvXI4Vkc"
   },
   "source": [
    "## 7. Model Training and Evaluation\n",
    "\n",
    "We train and evaluate the LPFormer model on the Marvel dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ow-Yx5G4Vkc"
   },
   "outputs": [],
   "source": [
    "evaluator = MRREvaluator()\n",
    "\n",
    "hyperparams = {\n",
    "    'hidden_dim': 128,\n",
    "    'learning_rate': 1e-3,\n",
    "    'decay': 0.95,\n",
    "    'dropout': 0.3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'ppr_threshold': 1e-3\n",
    "}\n",
    "\n",
    "print(f\"\\nUsing hyperparameters:\")\n",
    "for key, value in hyperparams.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BrIv15AbGn4"
   },
   "outputs": [],
   "source": [
    "print(\"Initializing LPFormer model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model = LPFormer(\n",
    "    num_nodes=data.num_nodes,\n",
    "    node_features=data.x,\n",
    "    train_edge_index=train_data.edge_index,\n",
    "    edge_index=data.edge_index,\n",
    "    hidden_dim=hyperparams['hidden_dim'],\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    dropout=hyperparams['dropout'],\n",
    "    ppr_threshold=hyperparams['ppr_threshold']\n",
    ").to(device)\n",
    "\n",
    "print(\"Initializing optimizer and scheduler...\")\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=hyperparams['learning_rate'],\n",
    "    weight_decay=hyperparams['weight_decay']\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma=hyperparams['decay']\n",
    ")\n",
    "\n",
    "initialization_time = time.time() - start_time\n",
    "print(f\"Model initialization completed in {initialization_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.1 Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePo2Gx0CbGn4"
   },
   "outputs": [],
   "source": [
    "best_val_metric = 0\n",
    "final_test_metric = 0\n",
    "num_epochs = 5  # Change\n",
    "patience = 5\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePo2Gx0CbGn4"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_metrics = []\n",
    "test_metrics = []\n",
    "val_aucs = []\n",
    "test_aucs = []\n",
    "val_aps = []\n",
    "test_aps = []\n",
    "epochs = []\n",
    "\n",
    "print(f\"Training LPFormer on Marvel dataset for {num_epochs} epochs...\")\n",
    "overall_start_time = time.time()\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    loss = train(model, optimizer, scheduler, data, split_edge)\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    results = test(model, data, split_edge, evaluator)\n",
    "    val_metric = results['valid']\n",
    "    test_metric = results['test']\n",
    "    val_metrics.append(val_metric)\n",
    "    test_metrics.append(test_metric)\n",
    "    val_aucs.append(results['valid_auc'])\n",
    "    test_aucs.append(results['test_auc'])\n",
    "    val_aps.append(results['valid_ap'])\n",
    "    test_aps.append(results['test_ap'])\n",
    "    epochs.append(epoch)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"\\nEpoch {epoch:02d} completed in {epoch_time:.2f} seconds\")\n",
    "    print(f\"Loss = {loss:.4f}\")\n",
    "    print(f\"Validation: MRR = {val_metric:.4f}, AUC = {results['valid_auc']:.4f}, AP = {results['valid_ap']:.4f}\")\n",
    "    print(f\"Test: MRR = {test_metric:.4f}, AUC = {results['test_auc']:.4f}, AP = {results['test_ap']:.4f}\")\n",
    "    \n",
    "    if val_metric > best_val_metric:\n",
    "        best_val_metric = val_metric\n",
    "        final_test_metric = test_metric\n",
    "        counter = 0\n",
    "        print(\"New best model! Saving model state...\")\n",
    "        torch.save(model.state_dict(), f\"lpformer_marvel_best.pt\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping after {epoch} epochs!\")\n",
    "            break\n",
    "\n",
    "total_training_time = time.time() - overall_start_time\n",
    "print(f\"Training completed in {total_training_time:.2f} seconds!\")\n",
    "print(f\"Best validation MRR: {best_val_metric:.4f}\")\n",
    "print(f\"Final test MRR: {final_test_metric:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBuWB10HbGn5"
   },
   "outputs": [],
   "source": [
    "print(\"Plotting training curves...\")\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, val_metrics, label='Validation MRR')\n",
    "plt.plot(epochs, test_metrics, label='Test MRR')\n",
    "plt.title('MRR Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MRR')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, val_aucs, label='Validation AUC')\n",
    "plt.plot(epochs, test_aucs, label='Test AUC')\n",
    "plt.title('AUC Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSaving experiment results...\")\n",
    "\n",
    "initialization_time = time.time() - start_time_project\n",
    "\n",
    "experiment_data = {\n",
    "    'num_nodes': data.num_nodes,\n",
    "    'num_edges': data.edge_index.size(1) // 2,\n",
    "    'running_time_seconds': round(initialization_time, 3) if initialization_time is not None else None,\n",
    "    'final_train_loss': round(train_losses[-1], 3) if train_losses else None,\n",
    "    'best_val_mrr': round(max(val_metrics), 3) if val_metrics else None,\n",
    "    'best_test_mrr': round(max(test_metrics), 3) if test_metrics else None,\n",
    "    'final_val_mrr': round(val_metrics[-1], 3) if val_metrics else None,\n",
    "    'final_test_mrr': round(test_metrics[-1], 3) if test_metrics else None,\n",
    "    'best_val_auc': round(max(val_aucs), 3) if val_aucs else None,\n",
    "    'best_test_auc': round(max(test_aucs), 3) if test_aucs else None,\n",
    "    'final_val_auc': round(val_aucs[-1], 3) if val_aucs else None,\n",
    "    'final_test_auc': round(test_aucs[-1], 3) if test_aucs else None,\n",
    "    'total_epochs': len(epochs) if epochs else None,\n",
    "    'graph_density': round((data.edge_index.size(1) // 2) / (data.num_nodes * (data.num_nodes - 1) / 2), 3),\n",
    "    'avg_degree': round((data.edge_index.size(1) / data.num_nodes), 3) if data.num_nodes > 0 else None\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([experiment_data])\n",
    "\n",
    "results_file = 'experiment_results.csv'\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    existing_df = pd.read_csv(results_file)\n",
    "    \n",
    "    duplicate_exists = False\n",
    "    if len(existing_df) > 0:\n",
    "        for idx, row in existing_df.iterrows():\n",
    "            if (row['num_nodes'] == experiment_data['num_nodes'] and \n",
    "                row['num_edges'] == experiment_data['num_edges'] and\n",
    "                abs(row.get('best_test_mrr', 0) - experiment_data.get('best_test_mrr', 0)) < 0.001):\n",
    "                duplicate_exists = True\n",
    "                print(f\"Duplicate experiment detected (row {idx+1}). Skipping save.\")\n",
    "                break\n",
    "    \n",
    "    if not duplicate_exists:\n",
    "        combined_df = pd.concat([existing_df, results_df], ignore_index=True)\n",
    "        combined_df.to_csv(results_file, index=False)\n",
    "        print(f\"Results appended to {results_file}\")\n",
    "        print(f\"Total experiments recorded: {len(combined_df)}\")\n",
    "    else:\n",
    "        print(\"Experiment already exists in results file.\")\n",
    "else:\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "    print(f\"Results saved to new file: {results_file}\")\n",
    "\n",
    "if not duplicate_exists if 'duplicate_exists' in locals() else True:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CURRENT EXPERIMENT SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Graph Size: {experiment_data['num_nodes']:,} nodes, {experiment_data['num_edges']:,} edges\")\n",
    "    print(f\"Density: {experiment_data['graph_density']:.6f}\")\n",
    "    print(f\"Average Degree: {experiment_data['avg_degree']:.2f}\")\n",
    "\n",
    "    if experiment_data['running_time_seconds']:\n",
    "        print(f\"Running Time: {experiment_data['running_time_seconds']:.2f} seconds\")\n",
    "\n",
    "    if experiment_data['final_val_mrr']:\n",
    "        print(f\"Final Validation MRR: {experiment_data['final_val_mrr']:.4f}\")\n",
    "        print(f\"Best Validation MRR: {experiment_data['best_val_mrr']:.4f}\")\n",
    "\n",
    "    if experiment_data['final_test_mrr']:\n",
    "        print(f\"Final Test MRR: {experiment_data['final_test_mrr']:.4f}\")\n",
    "        print(f\"Best Test MRR: {experiment_data['best_test_mrr']:.4f}\")\n",
    "\n",
    "    if experiment_data['final_val_auc']:\n",
    "        print(f\"Final Validation AUC: {experiment_data['final_val_auc']:.4f}\")\n",
    "        print(f\"Best Validation AUC: {experiment_data['best_val_auc']:.4f}\")\n",
    "\n",
    "    if experiment_data['final_test_auc']:\n",
    "        print(f\"Final Test AUC: {experiment_data['final_test_auc']:.4f}\")\n",
    "        print(f\"Best Test AUC: {experiment_data['best_test_auc']:.4f}\")\n",
    "\n",
    "    print(f\"Total Epochs: {experiment_data['total_epochs']}\")\n",
    "\n",
    "    if os.path.exists(results_file):\n",
    "        all_results = pd.read_csv(results_file)\n",
    "        \n",
    "        if len(all_results) > 1:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"EXPERIMENT COMPARISON\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            fig.suptitle('Experiment Results Comparison', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            if 'best_test_mrr' in all_results.columns:\n",
    "                ax1 = axes[0, 0]\n",
    "                ax1.scatter(all_results['num_nodes'], all_results['best_test_mrr'], \n",
    "                           c='blue', alpha=0.7, s=50)\n",
    "                ax1.set_xlabel('Number of Nodes')\n",
    "                ax1.set_ylabel('Best Test MRR')\n",
    "                ax1.set_title('Graph Size vs Performance (MRR)')\n",
    "                ax1.grid(True, alpha=0.3)\n",
    "                \n",
    "                current_idx = len(all_results) - 1\n",
    "                ax1.scatter(all_results.iloc[current_idx]['num_nodes'], \n",
    "                           all_results.iloc[current_idx]['best_test_mrr'],\n",
    "                           c='red', s=100, marker='*', label='Current')\n",
    "                ax1.legend()\n",
    "            \n",
    "            if 'best_test_auc' in all_results.columns:\n",
    "                ax2 = axes[0, 1]\n",
    "                ax2.scatter(all_results['num_nodes'], all_results['best_test_auc'], \n",
    "                           c='green', alpha=0.7, s=50)\n",
    "                ax2.set_xlabel('Number of Nodes')\n",
    "                ax2.set_ylabel('Best Test AUC')\n",
    "                ax2.set_title('Graph Size vs Performance (AUC)')\n",
    "                ax2.grid(True, alpha=0.3)\n",
    "                \n",
    "                ax2.scatter(all_results.iloc[current_idx]['num_nodes'], \n",
    "                           all_results.iloc[current_idx]['best_test_auc'],\n",
    "                           c='red', s=100, marker='*', label='Current')\n",
    "                ax2.legend()\n",
    "            \n",
    "            if 'running_time_seconds' in all_results.columns:\n",
    "                ax3 = axes[1, 0]\n",
    "                valid_times = all_results.dropna(subset=['running_time_seconds'])\n",
    "                if len(valid_times) > 0:\n",
    "                    ax3.scatter(valid_times['num_nodes'], valid_times['running_time_seconds'], \n",
    "                               c='orange', alpha=0.7, s=50)\n",
    "                    ax3.set_xlabel('Number of Nodes')\n",
    "                    ax3.set_ylabel('Running Time (seconds)')\n",
    "                    ax3.set_title('Graph Size vs Running Time')\n",
    "                    ax3.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    if experiment_data['running_time_seconds']:\n",
    "                        ax3.scatter(experiment_data['num_nodes'], \n",
    "                                   experiment_data['running_time_seconds'],\n",
    "                                   c='red', s=100, marker='*', label='Current')\n",
    "                        ax3.legend()\n",
    "            \n",
    "            if 'best_test_mrr' in all_results.columns:\n",
    "                ax4 = axes[1, 1]\n",
    "                ax4.scatter(all_results['graph_density'], all_results['best_test_mrr'], \n",
    "                           c='purple', alpha=0.7, s=50)\n",
    "                ax4.set_xlabel('Graph Density')\n",
    "                ax4.set_ylabel('Best Test MRR')\n",
    "                ax4.set_title('Density vs Performance')\n",
    "                ax4.grid(True, alpha=0.3)\n",
    "                \n",
    "                ax4.scatter(all_results.iloc[current_idx]['graph_density'], \n",
    "                           all_results.iloc[current_idx]['best_test_mrr'],\n",
    "                           c='red', s=100, marker='*', label='Current')\n",
    "                ax4.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nSummary across {len(all_results)} experiments:\")\n",
    "            print(f\"  Node range: {all_results['num_nodes'].min():,} - {all_results['num_nodes'].max():,}\")\n",
    "            print(f\"  Edge range: {all_results['num_edges'].min():,} - {all_results['num_edges'].max():,}\")\n",
    "            \n",
    "            if 'best_test_mrr' in all_results.columns:\n",
    "                print(f\"  Best Test MRR range: {all_results['best_test_mrr'].min():.4f} - {all_results['best_test_mrr'].max():.4f}\")\n",
    "            \n",
    "            if 'best_test_auc' in all_results.columns:\n",
    "                print(f\"  Best Test AUC range: {all_results['best_test_auc'].min():.4f} - {all_results['best_test_auc'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nExperiment logging complete!\")\n",
    "print(f\"  File location: {os.path.abspath(results_file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFur97embGn7"
   },
   "source": [
    "## 8. LP Factor Analysis\n",
    "\n",
    "We analyze the model's performance on different types of LP factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnDMTCtAbGn7"
   },
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"\\nPerforming LP factor analysis...\")\n",
    "try:\n",
    "    model.load_state_dict(torch.load(f\"lpformer_marvel_best.pt\"))\n",
    "    print(\"Loaded best model for analysis\")\n",
    "except:\n",
    "    print(\"Using current model for analysis (best model not found)\")\n",
    "\n",
    "# Analyze LP factors\n",
    "factor_results = analyze_lp_factors(model, data, split_edge)\n",
    "\n",
    "print(\"\\nLP Factor Analysis Results:\")\n",
    "for factor, score in factor_results.items():\n",
    "    print(f\"  {factor.capitalize()} factor: {score:.4f}\")\n",
    "\n",
    "print(\"Plotting factor analysis results...\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "factors = list(factor_results.keys())\n",
    "scores = [factor_results[f] for f in factors]\n",
    "\n",
    "plt.bar(factors, scores)\n",
    "plt.title('Performance by LP Factor Type')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    plt.text(i, score + 0.02, f\"{score:.4f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "new_section_id"
   },
   "source": [
    "## 9. Example Link Predictions\n",
    "\n",
    "We demonstrate the model's predictions on specific examples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example_predictions_id"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_top_links(model, data, node_idx, k=10, existing_edges=None, node_type_map=None):\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    \n",
    "    source_type = node_type_map[node_idx]\n",
    "    \n",
    "    if source_type == 'hero':\n",
    "        candidate_nodes = [idx for idx, node_type in node_type_map.items() \n",
    "                          if node_type == 'comic']\n",
    "    else:\n",
    "        candidate_nodes = [idx for idx, node_type in node_type_map.items() \n",
    "                          if node_type == 'hero']\n",
    "    \n",
    "    candidate_nodes = torch.tensor(candidate_nodes, device=device)\n",
    "    \n",
    "    candidate_links = torch.stack([\n",
    "        torch.ones_like(candidate_nodes) * node_idx,\n",
    "        candidate_nodes\n",
    "    ], dim=1)\n",
    "    \n",
    "    if existing_edges is not None:\n",
    "        existing_set = set()\n",
    "        for i in range(existing_edges.size(0)):\n",
    "            src, dst = existing_edges[i]\n",
    "            existing_set.add((src.item(), dst.item()))\n",
    "            existing_set.add((dst.item(), src.item()))  # Undirected graph\n",
    "        \n",
    "        filtered_links = []\n",
    "        for i in range(candidate_links.size(0)):\n",
    "            src, dst = candidate_links[i]\n",
    "            if (src.item(), dst.item()) not in existing_set:\n",
    "                filtered_links.append(candidate_links[i])\n",
    "        \n",
    "        if len(filtered_links) > 0:\n",
    "            candidate_links = torch.stack(filtered_links)\n",
    "    \n",
    "    batch_size = 64\n",
    "    all_scores = []\n",
    "    \n",
    "    for i in range(0, candidate_links.size(0), batch_size):\n",
    "        batch_links = candidate_links[i:i+batch_size]\n",
    "        batch_scores = model(data.x, data.edge_index, batch_links)\n",
    "        all_scores.append(batch_scores)\n",
    "    \n",
    "    if len(all_scores) > 0:\n",
    "        all_scores = torch.cat(all_scores)\n",
    "        \n",
    "        if all_scores.size(0) > k:\n",
    "            top_k_values, top_k_indices = torch.topk(all_scores, k)\n",
    "            top_k_links = candidate_links[top_k_indices]\n",
    "            return top_k_links[:, 1], top_k_values\n",
    "        else:\n",
    "            return candidate_links[:, 1], all_scores\n",
    "    else:\n",
    "        return torch.tensor([], device=device), torch.tensor([], device=device)\n",
    "def get_node_name(idx, nodes_df, idx_to_node):\n",
    "    node_id = idx_to_node[idx]\n",
    "    node_type = nodes_df[nodes_df['node'] == node_id]['type'].values[0]\n",
    "    return f\"{node_id} ({node_type})\"\n",
    "\n",
    "print(\"\\nGenerating example link predictions...\")\n",
    "try:\n",
    "    model.load_state_dict(torch.load(f\"lpformer_marvel_best.pt\"))\n",
    "    print(\"Loaded best model for predictions\")\n",
    "except:\n",
    "    print(\"Using current model for predictions (best model not found)\")\n",
    "\n",
    "existing_edges = data.edge_index.t()\n",
    "\n",
    "hero_indices = nodes_df[nodes_df['type'] == 'hero']['node_idx'].values\n",
    "comic_indices = nodes_df[nodes_df['type'] == 'comic']['node_idx'].values\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "example_heroes = np.random.choice(hero_indices, 3)\n",
    "example_comics = np.random.choice(comic_indices, 3)\n",
    "example_nodes = np.concatenate([example_heroes, example_comics])\n",
    "\n",
    "for node_idx in example_nodes:\n",
    "    node_name = get_node_name(node_idx, nodes_df, idx_to_node)\n",
    "    print(f\"\\nTop 5 predicted links for {node_name}:\")\n",
    "    \n",
    "    target_nodes, scores = predict_top_links(\n",
    "        model, data, node_idx, k=5, \n",
    "        existing_edges=existing_edges,\n",
    "        node_type_map=node_type_map  # Add this parameter\n",
    "    )\n",
    "    if len(target_nodes) > 0:\n",
    "        for i, (target, score) in enumerate(zip(target_nodes, scores)):\n",
    "            target_name = get_node_name(target.item(), nodes_df, idx_to_node)\n",
    "            print(f\"  {i+1}. {target_name} (Score: {score.item():.4f})\")\n",
    "    else:\n",
    "        print(\"  No predictions available (all nodes are already connected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualization_id"
   },
   "outputs": [],
   "source": [
    "def create_node_type_map(nodes_df, idx_to_node):\n",
    "    node_type_map = {}\n",
    "    for idx, node_name in idx_to_node.items():\n",
    "        node_row = nodes_df[nodes_df['node'] == node_name]\n",
    "        if not node_row.empty:\n",
    "            node_type_map[idx] = node_row['type'].iloc[0]\n",
    "        else:\n",
    "            node_type_map[idx] = 'hero' if 'hero' in node_name.lower() else 'comic'\n",
    "    return node_type_map\n",
    "\n",
    "node_type_map = create_node_type_map(nodes_df, idx_to_node)\n",
    "\n",
    "# Fixed visualize_predictions function\n",
    "def visualize_predictions(model, data, node_idx, k=5, existing_edges=None, node_type_map=None):\n",
    "    target_nodes, scores = predict_top_links(\n",
    "        model, data, node_idx, k=k, existing_edges=existing_edges, node_type_map=node_type_map\n",
    "    )\n",
    "    \n",
    "    if len(target_nodes) == 0:\n",
    "        print(\"No predictions available for visualization\")\n",
    "        return\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    source_name = get_node_name(node_idx, nodes_df, idx_to_node)\n",
    "    source_type = node_type_map[node_idx] if node_type_map else ('hero' if 'hero' in source_name.lower() else 'comic')\n",
    "    G.add_node(source_name, type=source_type)\n",
    "    \n",
    "    for target, score in zip(target_nodes, scores):\n",
    "        target_name = get_node_name(target.item(), nodes_df, idx_to_node)\n",
    "        target_type = node_type_map[target.item()] if node_type_map else ('hero' if 'hero' in target_name.lower() else 'comic')\n",
    "        G.add_node(target_name, type=target_type)\n",
    "        G.add_edge(source_name, target_name, weight=score.item())\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    pos = nx.spring_layout(G, seed=42, k=2, iterations=50)  # Better layout\n",
    "    \n",
    "    hero_nodes = [n for n, attr in G.nodes(data=True) if attr['type'] == 'hero']\n",
    "    comic_nodes = [n for n, attr in G.nodes(data=True) if attr['type'] == 'comic']\n",
    "    \n",
    "    if hero_nodes:\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=hero_nodes, node_color='skyblue', \n",
    "                              node_size=500, alpha=0.8, label='Heroes')\n",
    "    \n",
    "    if comic_nodes:\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=comic_nodes, node_color='lightgreen', \n",
    "                              node_size=500, alpha=0.8, label='Comics')\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[source_name], node_color='red', \n",
    "                          node_size=700, alpha=1.0, label='Source')\n",
    "    \n",
    "    edge_widths = [d['weight']*5 for u, v, d in G.edges(data=True)]\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.7, edge_color='gray')\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "    \n",
    "    edge_labels = {(u, v): f\"{d['weight']:.3f}\" for u, v, d in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7)\n",
    "    \n",
    "    plt.title(f\"Top {k} Predicted Links for {source_name}\", fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nVisualizing predictions for a hero:\")\n",
    "hero_idx = example_heroes[0]\n",
    "visualize_predictions(model, data, hero_idx, k=5, existing_edges=existing_edges, node_type_map=node_type_map)\n",
    "\n",
    "print(\"\\nVisualizing predictions for a comic:\")\n",
    "comic_idx = example_comics[0] \n",
    "visualize_predictions(model, data, comic_idx, k=5, existing_edges=existing_edges, node_type_map=node_type_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
